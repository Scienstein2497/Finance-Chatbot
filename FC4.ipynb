{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafb7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import sqlite3\n",
    "import base64\n",
    "import openai\n",
    "\n",
    "# Monkey patching to fix the issue\n",
    "if st.__version__ == \"0.86.0\":\n",
    "    st.button = st.experimental_singleton(st.button)\n",
    "\n",
    "# Set your OpenAI API key here\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Function to upload Parquet file on Streamlit\n",
    "def upload_parquet_file():\n",
    "    uploaded_file = st.file_uploader(\"Choose a Parquet file\", type=[\"parquet\"], key=\"parquet_file_upload\")\n",
    "    return uploaded_file\n",
    "\n",
    "# Function to load Parquet file\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_parquet_file(file):\n",
    "    # Use pyarrow to read Parquet file\n",
    "    table = pq.read_table(file)\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "    return df\n",
    "\n",
    "# Function to view contents of a Parquet file\n",
    "def view_parquet_contents(parquet_file):\n",
    "    # Use pandas to read Parquet file without 'snappy' codec\n",
    "    df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "    return df\n",
    "\n",
    "# Function to apply data transformations based on a JSON file\n",
    "def apply_transformations(df, transformation_json):\n",
    "    with st.spinner(\"Applying Transformations...\"):\n",
    "        try:\n",
    "            # Check if transformation_json is a file object\n",
    "            if hasattr(transformation_json, 'read'):\n",
    "                # If it's a file object, read its content\n",
    "                transformation_json = transformation_json.read().decode(\"utf-8\")\n",
    "\n",
    "            # Check if the JSON content is empty\n",
    "            if not transformation_json.strip():\n",
    "                st.warning(\"Transformation JSON content is empty.\")\n",
    "                # Drop empty column here\n",
    "                df = df.dropna(axis=1, how='all')\n",
    "                return df\n",
    "\n",
    "            # Print the content of transformation_json for debugging\n",
    "            st.write(\"Transformation JSON Content:\")\n",
    "            st.code(transformation_json)\n",
    "\n",
    "            # Load transformation rules from JSON\n",
    "            transformations = json.loads(transformation_json)\n",
    "\n",
    "            # Apply transformations to DataFrame\n",
    "            for transformation in transformations:\n",
    "                # Implement your specific transformations here\n",
    "                if transformation['type'] == 'convert_data_type':\n",
    "                    column_to_convert = transformation['column']\n",
    "                    new_data_type = transformation['new_data_type']\n",
    "                    date_format = transformation.get('date_format')  # Optional date_format for datetime conversion\n",
    "                    # Convert data type\n",
    "                    if new_data_type == \"datetime\" and date_format:\n",
    "                        df[column_to_convert] = pd.to_datetime(df[column_to_convert], errors='coerce', format=date_format)\n",
    "                    else:\n",
    "                        df[column_to_convert] = df[column_to_convert].astype(new_data_type)\n",
    "\n",
    "                elif transformation['type'] == 'map_values':\n",
    "                    column_to_map = transformation['column']\n",
    "                    value_mapping = transformation['value_mapping']\n",
    "                    # Map values\n",
    "                    df[column_to_map] = df[column_to_map].map(value_mapping)\n",
    "\n",
    "                elif transformation['type'] == 'extract_month':\n",
    "                    column_to_extract = transformation['column']\n",
    "                    new_column_name = transformation['new_column']\n",
    "                    # Extract month\n",
    "                    df[new_column_name] = pd.to_datetime(df[column_to_extract], errors='coerce').dt.month\n",
    "\n",
    "                elif transformation['type'] == 'drop_columns':\n",
    "                    columns_to_drop = transformation['columns']\n",
    "                    # Drop columns\n",
    "                    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "                elif transformation['type'] == 'filter_rows':\n",
    "                    filter_condition = transformation['condition']\n",
    "                    # Filter rows based on condition\n",
    "                    df = df.query(filter_condition)\n",
    "\n",
    "                # Additional transformations for Parquet File 2\n",
    "                elif transformation['type'] == 'rename_column':\n",
    "                    old_column_name = transformation['old_column']\n",
    "                    new_column_name = transformation['new_column']\n",
    "                    # Rename column\n",
    "                    df = df.rename(columns={old_column_name: new_column_name}, errors='ignore')\n",
    "\n",
    "                elif transformation['type'] == 'replace_values':\n",
    "                    column_to_replace = transformation['column']\n",
    "                    value_mapping = transformation['value_mapping']\n",
    "                    # Replace values\n",
    "                    df[column_to_replace] = df[column_to_replace].replace(value_mapping)\n",
    "\n",
    "                elif transformation['type'] == 'fill_missing_values':\n",
    "                    column_to_fill = transformation['column']\n",
    "                    fill_value = transformation['fill_value']\n",
    "                    # Fill missing values\n",
    "                    df[column_to_fill] = df[column_to_fill].fillna(fill_value)\n",
    "\n",
    "                elif transformation['type'] == 'normalize_column':\n",
    "                    column_to_normalize = transformation['column']\n",
    "                    # Normalize column\n",
    "                    df[column_to_normalize] = (df[column_to_normalize] - df[column_to_normalize].mean()) / df[column_to_normalize].std()\n",
    "\n",
    "                # Additional transformations for Parquet File 3\n",
    "                elif transformation['type'] == 'round_values':\n",
    "                    column_to_round = transformation['column']\n",
    "                    decimal_places = transformation['decimal_places']\n",
    "                    # Round values\n",
    "                    df[column_to_round] = df[column_to_round].round(decimal_places)\n",
    "\n",
    "                elif transformation['type'] == 'calculate_age':\n",
    "                    column_birth_year = transformation['column']\n",
    "                    new_column_name = transformation['new_column']\n",
    "                    # Calculate age\n",
    "                    df[new_column_name] = pd.to_datetime('today').year - df[column_birth_year]\n",
    "\n",
    "                elif transformation['type'] == 'encode_categorical':\n",
    "                    column_to_encode = transformation['column']\n",
    "                    # Encode categorical column\n",
    "                    df = pd.get_dummies(df, columns=[column_to_encode], prefix=[column_to_encode])\n",
    "\n",
    "\n",
    "            # Add more transformations as needed\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            st.error(f\"Error decoding JSON content: {e}\")\n",
    "            return df\n",
    "\n",
    "    st.session_state.df_transformed = df.copy()  # Save the transformed DataFrame in session state\n",
    "    return df\n",
    "\n",
    "# Function to export data to SQL Database\n",
    "def export_to_sql(df, connection, table_name, create_table=True):\n",
    "    with st.spinner(\"Exporting Data to SQL Database...\"):\n",
    "        if create_table:\n",
    "            df.to_sql(table_name, connection, index=False, if_exists='replace')\n",
    "        else:\n",
    "            df.to_sql(table_name, connection, index=False, if_exists='append')\n",
    "\n",
    "# Function to export data to CSV\n",
    "def export_to_csv(df, file_name):\n",
    "    with st.spinner(\"Exporting Data to CSV...\"):\n",
    "        df.to_csv(file_name, index=False)\n",
    "        st.success(f\"Data exported to CSV file '{file_name}'.\")\n",
    "\n",
    "# Function to describe the sample dataset using ChatGPT API\n",
    "def describe_dataset_with_chatgpt(df):\n",
    "    with st.spinner(\"Describing Sample Dataset...\"):\n",
    "        # Take a sample of the dataset for analysis (adjust sample size as needed)\n",
    "        sample_size = min(len(df), 100)\n",
    "        sample_data = df.sample(sample_size)\n",
    "\n",
    "        # Create a text prompt for ChatGPT with a shorter sample data\n",
    "        prompt = f\"Describe the dataset:\\n\\n{sample_data.head().to_markdown(index=False)}\"\n",
    "\n",
    "        # Make a request to the ChatGPT API with a smaller completion length\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-002\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=100,  # Adjust this value to fit within the model's maximum context length\n",
    "            temperature=0.7,\n",
    "            n=1\n",
    "        )\n",
    "\n",
    "        # Extract and display the generated response\n",
    "        generated_text = response['choices'][0]['text']\n",
    "        st.subheader(\"Sample Dataset Description:\")\n",
    "        st.markdown(generated_text)\n",
    "\n",
    "# Streamlit App\n",
    "def main():\n",
    "    st.title(\"Finance Fraud Detection - Data Upload, Display, and Transformations\")\n",
    "\n",
    "    # Data Upload Section\n",
    "    st.header(\"Upload Parquet File\")\n",
    "\n",
    "    # Upload Parquet file\n",
    "    parquet_file = upload_parquet_file()\n",
    "\n",
    "    if parquet_file is not None:\n",
    "        # Display first few rows of the uploaded Parquet data\n",
    "        # Check if df_parquet is already loaded\n",
    "        if 'df_parquet' not in st.session_state:\n",
    "            df_parquet = load_parquet_file(parquet_file)\n",
    "            st.session_state.df_parquet = df_parquet  # Store in session state to avoid duplicate widget ID error\n",
    "        else:\n",
    "            df_parquet = st.session_state.df_parquet\n",
    "\n",
    "        st.subheader(\"Preview of Uploaded Parquet Data\")\n",
    "        st.dataframe(df_parquet.head())\n",
    "\n",
    "        transformation_file = st.file_uploader(\"Choose a Transformation JSON file\", type=[\"json\"], key=\"transformations\")\n",
    "\n",
    "        # Display Transformation JSON as a table\n",
    "        if transformation_file is not None:\n",
    "            st.subheader(\"Preview of Uploaded Transformation JSON Data\")\n",
    "            df_json = pd.read_json(transformation_file)\n",
    "            st.dataframe(df_json)\n",
    "\n",
    "        # Apply Transformations button\n",
    "        if st.button(\"Apply Transformations\"):\n",
    "            # Read content of the uploaded file\n",
    "            transformation_json = None\n",
    "            if transformation_file is not None:\n",
    "                transformation_json = transformation_file.read().decode(\"utf-8\")\n",
    "\n",
    "            # Apply transformations\n",
    "            df_transformed = apply_transformations(df_parquet, transformation_json)\n",
    "\n",
    "            # Display transformed data\n",
    "            st.subheader(\"Transformed Data:\")\n",
    "            st.dataframe(df_transformed)\n",
    "\n",
    "            # Export to SQL Database Section\n",
    "            st.header(\"Export to SQL Database\")\n",
    "\n",
    "            # SQL Connection\n",
    "            connection = sqlite3.connect(\":memory:\")\n",
    "\n",
    "            # Options for exporting to SQL\n",
    "            export_option = st.selectbox(\"Choose an option:\", [\"Create table and insert data\", \"Insert into an already existing table\"])\n",
    "\n",
    "            # Custom table name input\n",
    "            custom_table_name = st.text_input(\"Enter a custom table name:\")\n",
    "\n",
    "            # Export button\n",
    "            if st.button(\"Export to SQL Database\"):\n",
    "                try:\n",
    "                    if export_option == \"Create table and insert data\":\n",
    "                        table_name = custom_table_name if custom_table_name else \"default_table\"\n",
    "                        export_to_sql(df_transformed, connection, table_name, create_table=True)\n",
    "                        st.success(f\"Data exported to SQL table '{table_name}'.\")\n",
    "                    elif export_option == \"Insert into an already existing table\":\n",
    "                        existing_table_name = custom_table_name if custom_table_name else \"default_table\"\n",
    "                        export_to_sql(df_transformed, connection, existing_table_name, create_table=False)\n",
    "                        st.success(f\"Data inserted into SQL table '{existing_table_name}'.\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error exporting to SQL Database: {e}\")\n",
    "\n",
    "            # Export Transformed Data Section\n",
    "            st.header(\"Download Transformed Data\")\n",
    "\n",
    "            # CSV file name input for transformed data\n",
    "            transformed_csv_file_name = st.text_input(\"Enter a CSV file name for Transformed Data:\")\n",
    "\n",
    "            # Export button for transformed data\n",
    "            if st.button(\"Download Transformed Data as CSV\"):\n",
    "                try:\n",
    "                    export_to_csv(df_transformed, f\"{transformed_csv_file_name}.csv\")\n",
    "                    st.success(f\"Transformed Data exported to CSV file '{transformed_csv_file_name}.csv'.\")\n",
    "                    st.markdown(get_download_link(df_transformed, f\"{transformed_csv_file_name}.csv\"), unsafe_allow_html=True)\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error exporting Transformed Data to CSV: {e}\")\n",
    "\n",
    "    # Describe Sample Dataset with ChatGPT API Section\n",
    "    st.header(\"Describe Sample Dataset with ChatGPT API\")\n",
    "\n",
    "    # Describe button\n",
    "    if st.button(\"Describe Cleaned Dataset\"):\n",
    "        if 'df_transformed' in st.session_state:\n",
    "            # If the transformed dataset is available, describe it\n",
    "            describe_dataset_with_chatgpt(st.session_state.df_transformed)\n",
    "        elif 'df_parquet' in st.session_state:\n",
    "            # If only the original dataset is available, describe it\n",
    "            describe_dataset_with_chatgpt(st.session_state.df_parquet)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
